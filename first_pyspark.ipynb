{"cells": [{"cell_type": "code", "execution_count": 1, "id": "71cfd779-0952-47fe-9ddc-39c9d5646fb7", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/08/27 03:15:14 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}, {"name": "stdout", "output_type": "stream", "text": "root\n |-- Product: string (nullable = true)\n |-- ID: integer (nullable = true)\n |-- Date: date (nullable = true)\n |-- Timestamp: timestamp (nullable = true)\n |-- Price: float (nullable = true)\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+---------+----+----------+-------------------+-----+\n|  Product|  ID|      Date|          Timestamp|Price|\n+---------+----+----------+-------------------+-----+\n|Product A|1001|2023-07-20|2023-07-20 10:15:30|29.99|\n|Product B|1002|2023-07-19|2023-07-19 14:20:45|49.99|\n|Product C|1003|2023-07-18|2023-07-18 09:30:15|39.99|\n|Product D|1004|2023-07-17|2023-07-17 16:45:00|19.99|\n+---------+----+----------+-------------------+-----+\n\n"}], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\nfrom datetime import datetime\n\n\nspark = SparkSession.builder \\\n    .appName(\"Spark with Hive\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()\n\n\ndata = [\n    [\"Product A\", 1001, datetime.strptime(\"2023-07-20\", \"%Y-%m-%d\"), datetime.strptime(\"2023-07-20 10:15:30\", \"%Y-%m-%d %H:%M:%S\"), 29.99],\n    [\"Product B\", 1002, datetime.strptime(\"2023-07-19\", \"%Y-%m-%d\"), datetime.strptime(\"2023-07-19 14:20:45\", \"%Y-%m-%d %H:%M:%S\"), 49.99],\n    [\"Product C\", 1003, datetime.strptime(\"2023-07-18\", \"%Y-%m-%d\"), datetime.strptime(\"2023-07-18 09:30:15\", \"%Y-%m-%d %H:%M:%S\"), 39.99],\n    [\"Product D\", 1004, datetime.strptime(\"2023-07-17\", \"%Y-%m-%d\"), datetime.strptime(\"2023-07-17 16:45:00\", \"%Y-%m-%d %H:%M:%S\"), 19.99]\n]\n\n\nschema = StructType([\n    StructField(\"Product\", StringType(), True),\n    StructField(\"ID\", IntegerType(), True),\n    StructField(\"Date\", DateType(), True),\n    StructField(\"Timestamp\", TimestampType(), True),\n    StructField(\"Price\", FloatType(), True)\n])\n\n\ndf = spark.createDataFrame(data, schema)\n\n\ndf.printSchema()\n\n\ndf.show()"}, {"cell_type": "code", "execution_count": 3, "id": "6dc4ab06-8edf-49a4-8f8f-db0ac03c211b", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- order_id: string (nullable = true)\n |-- order_item_id: integer (nullable = true)\n |-- product_id: string (nullable = true)\n |-- seller_id: string (nullable = true)\n |-- shipping_limit_date: timestamp (nullable = true)\n |-- price: double (nullable = true)\n |-- freight_value: double (nullable = true)\n\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 2:>                                                          (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|\n+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.9|        13.29|\n|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13|239.9|        19.93|\n|000229ec398224ef6...|            1|c777355d18b72b67a...|5b51032eddd242adc...|2018-01-18 14:48:30|199.0|        17.87|\n|00024acbcdf0a6daa...|            1|7634da152a4610f15...|9d7a1d34a50524090...|2018-08-15 10:10:18|12.99|        12.79|\n|00042b26cf59d7ce6...|            1|ac6c3623068f30de0...|df560393f3a51e745...|2017-02-13 13:57:51|199.9|        18.14|\n+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "schema = StructType([\n    StructField(\"order_id\", StringType(), True),\n    StructField(\"order_item_id\", IntegerType(), True),\n    StructField(\"product_id\", StringType(), True),\n    StructField(\"seller_id\", StringType(), True),\n    StructField(\"shipping_limit_date\", TimestampType(), True),\n    StructField(\"price\", DoubleType(), True),\n    StructField(\"freight_value\", DoubleType(), True)\n])\n\nhdfs_path = '/tmp/input_data/order_items_dataset.csv'\ndf = spark.read.format('csv').option('header', 'true').option('inferSchema', 'false').schema(schema).load(hdfs_path)\n\ndf.printSchema()\n\ndf.show(5)"}, {"cell_type": "code", "execution_count": 4, "id": "fe49205e-1e65-4d37-b937-6e482757cda1", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "root\n |-- order_id: string (nullable = true)\n |-- order_item_id: integer (nullable = true)\n |-- product_id: string (nullable = true)\n |-- seller_id: string (nullable = true)\n |-- shipping_limit_date: timestamp (nullable = true)\n |-- price: double (nullable = true)\n |-- freight_value: double (nullable = true)\n\n+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|\n+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.9|        13.29|\n|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13|239.9|        19.93|\n|000229ec398224ef6...|            1|c777355d18b72b67a...|5b51032eddd242adc...|2018-01-18 14:48:30|199.0|        17.87|\n|00024acbcdf0a6daa...|            1|7634da152a4610f15...|9d7a1d34a50524090...|2018-08-15 10:10:18|12.99|        12.79|\n|00042b26cf59d7ce6...|            1|ac6c3623068f30de0...|df560393f3a51e745...|2017-02-13 13:57:51|199.9|        18.14|\n+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\nonly showing top 5 rows\n\n"}], "source": "hdfs_path = '/tmp/input_data/order_items_dataset.csv'\ndf2 = spark.read.format('csv').option('header', 'true').option('inferSchema', 'true').load(hdfs_path)\n\n\ndf2.printSchema()\ndf2.show(5)"}, {"cell_type": "code", "execution_count": 5, "id": "76714772-8c58-4d22-ac1b-aa8b2cb6f70c", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Number of partitions: 2\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 6:=============================>                             (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "Number of partitions: 10\n"}], "source": "print(f'Number of partitions: {df2.rdd.getNumPartitions()}')\n\ndf3 = df2.repartition(10)\n\n\nprint(f'Number of partitions: {df3.rdd.getNumPartitions()}')"}, {"cell_type": "code", "execution_count": 6, "id": "4cf824af-c46c-47b8-b77c-a59e81e0cc21", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+\n|            order_id|\n+--------------------+\n|6299bb8e855289b41...|\n|71fbb9971d84bf97a...|\n|74322a01b770c2ea3...|\n|a23fc2b3af4f1a48e...|\n|747af114bbea56ac1...|\n+--------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------------+\n|            order_id|shipping_limit_date|\n+--------------------+-------------------+\n|3bbf8f927f288e4a1...|2017-11-09 14:25:38|\n|50c40cfcbb6ce3fca...|2018-06-14 09:52:04|\n|51c3d73e0e9052253...|2018-02-22 19:15:27|\n|183ee0e3ebd4c1c99...|2018-02-07 20:14:08|\n|3a1400b5d4dd3082a...|2018-03-27 17:28:20|\n+--------------------+-------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------------+\n|            order_id|shipping_limit_date|\n+--------------------+-------------------+\n|f0b47dadd5f372c41...|2018-05-14 04:54:53|\n|bbd319ae8e4b46101...|2017-11-24 02:28:04|\n|e16fb24453a306d5d...|2018-08-10 03:24:54|\n|d4de6d0debe2df72c...|2017-03-13 03:35:12|\n|bdbe8da70dcc6e6a2...|2018-04-22 21:52:25|\n+--------------------+-------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 16:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------------+\n|                 oid|         limit_date|\n+--------------------+-------------------+\n|3bbf8f927f288e4a1...|2017-11-09 14:25:38|\n|50c40cfcbb6ce3fca...|2018-06-14 09:52:04|\n|51c3d73e0e9052253...|2018-02-22 19:15:27|\n|183ee0e3ebd4c1c99...|2018-02-07 20:14:08|\n|3a1400b5d4dd3082a...|2018-03-27 17:28:20|\n+--------------------+-------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import *\n\ndf3.select('order_id').show(5)\ndf3.select('order_id', 'shipping_limit_date').show(5)\ndf3.select(col('order_id'), col('shipping_limit_date')).show(5)\ndf3.select(col('order_id').alias('oid'), col('shipping_limit_date').alias('limit_date')).show(5)"}, {"cell_type": "code", "execution_count": 7, "id": "984f85d3-8d59-46cf-85b9-b84046dd102f", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 19:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------------+----+-----+\n|            order_id|shipping_limit_date|year|month|\n+--------------------+-------------------+----+-----+\n|3bbf8f927f288e4a1...|2017-11-09 14:25:38|2017|   11|\n|50c40cfcbb6ce3fca...|2018-06-14 09:52:04|2018|    6|\n|51c3d73e0e9052253...|2018-02-22 19:15:27|2018|    2|\n|183ee0e3ebd4c1c99...|2018-02-07 20:14:08|2018|    2|\n|3a1400b5d4dd3082a...|2018-03-27 17:28:20|2018|    3|\n+--------------------+-------------------+----+-----+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df4 = df3.withColumn(\"year\", year(col(\"shipping_limit_date\"))).withColumn(\"month\", month(col(\"shipping_limit_date\")))\n\ndf4.select(\"order_id\", \"shipping_limit_date\", \"year\", \"month\").show(5)"}, {"cell_type": "code", "execution_count": 8, "id": "8cc1622a-3475-4718-87c8-b99505379291", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 22:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-----------------------+\n|            order_id|shipping_limit_datetime|\n+--------------------+-----------------------+\n|3bbf8f927f288e4a1...|    2017-11-09 14:25:38|\n|50c40cfcbb6ce3fca...|    2018-06-14 09:52:04|\n|51c3d73e0e9052253...|    2018-02-22 19:15:27|\n|183ee0e3ebd4c1c99...|    2018-02-07 20:14:08|\n|3a1400b5d4dd3082a...|    2018-03-27 17:28:20|\n+--------------------+-----------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df5 = df4.withColumnRenamed('shipping_limit_date', 'shipping_limit_datetime')\n\ndf5.select(\"order_id\", \"shipping_limit_datetime\").show(5)"}, {"cell_type": "code", "execution_count": 9, "id": "c4bc0fed-ed71-49c2-9ea6-f4f05779dd44", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime|price|freight_value|year|month|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|    2017-09-19 09:45:35| 58.9|        13.29|2017|    9|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime|price|freight_value|year|month|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|    2017-09-19 09:45:35| 58.9|        13.29|2017|    9|\n|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|    2017-05-03 11:05:13|239.9|        19.93|2017|    5|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime|price|freight_value|year|month|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n|363524b17966c3a64...|            2|43ee88561093499d9...|23613d49c3ac2bd30...|    2018-05-24 22:35:14| 10.9|          3.8|2018|    5|\n|1d9609dad08db33f3...|            1|7cc67695a7648efc5...|95e03ca3d4146e401...|    2017-12-11 18:10:31|29.99|          8.9|2017|   12|\n|50aff4b82439e01c5...|            1|ec1faa2edc27ce323...|cc419e0650a3c5ba7...|    2017-11-23 21:53:21|29.99|         7.78|2017|   11|\n|37ee401157a3a0b28...|            9|d34c07a2d817ac73f...|e7d5b006eb624f130...|    2018-04-19 02:30:52|29.99|         7.39|2018|    4|\n|8f5fac100b291e3c7...|            1|0e996644bf2835621...|b4ffb71f0cb1b1c3d...|    2017-12-08 09:13:27| 6.84|         7.78|2017|   12|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\nonly showing top 5 rows\n\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime|price|freight_value|year|month|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n|363524b17966c3a64...|            2|43ee88561093499d9...|23613d49c3ac2bd30...|    2018-05-24 22:35:14| 10.9|          3.8|2018|    5|\n|1d9609dad08db33f3...|            1|7cc67695a7648efc5...|95e03ca3d4146e401...|    2017-12-11 18:10:31|29.99|          8.9|2017|   12|\n|50aff4b82439e01c5...|            1|ec1faa2edc27ce323...|cc419e0650a3c5ba7...|    2017-11-23 21:53:21|29.99|         7.78|2017|   11|\n|37ee401157a3a0b28...|            9|d34c07a2d817ac73f...|e7d5b006eb624f130...|    2018-04-19 02:30:52|29.99|         7.39|2018|    4|\n|8f5fac100b291e3c7...|            1|0e996644bf2835621...|b4ffb71f0cb1b1c3d...|    2017-12-08 09:13:27| 6.84|         7.78|2017|   12|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\nonly showing top 5 rows\n\n"}], "source": "df5.filter(col(\"order_id\") == '00010242fe8c5a6d1ba2dd792cb16214').show(5)\n\norder_li = ['00010242fe8c5a6d1ba2dd792cb16214','00018f77f2f0320c557190d7a144bdd3']\ndf5.filter(col(\"order_id\").isin(order_li)).show(5)\n\ndf5.filter((col(\"price\") < 50) & (col(\"freight_value\")  < 10)).show(5)\n\n\ndf5.filter(\"price < 50 and freight_value  < 10\").show(5)"}, {"cell_type": "code", "execution_count": 10, "id": "4363a358-dc04-41a2-afa9-049f19a468eb", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 37:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime| price|freight_value|year|month|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+\n|1e1bb536916a99649...|            2|0288f8dd74b931b4e...|1da3aeb70d7989d1e...|    2017-09-05 12:10:11| 49.99|        21.15|2017|    9|\n|62a0e822dd605871a...|            1|31dbb0d1815bdc83c...|6da1992f915d77be9...|    2017-06-08 11:50:18|  29.0|        15.79|2017|    6|\n|025c72e88fbf2358b...|            2|bef21943bc2335188...|e49c26c3edfa46d22...|    2017-03-21 21:24:27|  19.9|         20.8|2017|    3|\n|23d16dddab46fd3d0...|            1|cca8e09ba6f2d35e4...|43f8c9950d11ecd03...|    2018-01-31 22:17:51|109.99|        14.52|2018|    1|\n|71c0d1686c9b55563...|            2|eb6c2ecde53034fc9...|1025f0e2d44d7041d...|    2017-12-01 19:31:45| 32.99|        16.11|2017|   12|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df5.show(5)"}, {"cell_type": "code", "execution_count": 11, "id": "61b4474a-fb77-422a-ada9-d703c8e012d1", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 40:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime| price|freight_value|year|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+\n|1e1bb536916a99649...|            2|0288f8dd74b931b4e...|1da3aeb70d7989d1e...|    2017-09-05 12:10:11| 49.99|        21.15|2017|\n|62a0e822dd605871a...|            1|31dbb0d1815bdc83c...|6da1992f915d77be9...|    2017-06-08 11:50:18|  29.0|        15.79|2017|\n|025c72e88fbf2358b...|            2|bef21943bc2335188...|e49c26c3edfa46d22...|    2017-03-21 21:24:27|  19.9|         20.8|2017|\n|23d16dddab46fd3d0...|            1|cca8e09ba6f2d35e4...|43f8c9950d11ecd03...|    2018-01-31 22:17:51|109.99|        14.52|2018|\n|71c0d1686c9b55563...|            2|eb6c2ecde53034fc9...|1025f0e2d44d7041d...|    2017-12-01 19:31:45| 32.99|        16.11|2017|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df5.drop('month').show(5)"}, {"cell_type": "code", "execution_count": 12, "id": "bfc7d621-880f-4879-9a14-de3afac57713", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 48:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime| price|freight_value|year|month|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+\n|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|    2017-05-03 11:05:13| 239.9|        19.93|2017|    5|\n|000229ec398224ef6...|            1|c777355d18b72b67a...|5b51032eddd242adc...|    2018-01-18 14:48:30| 199.0|        17.87|2018|    1|\n|00048cc3ae777c65d...|            1|ef92defde845ab845...|6426d21aca402a131...|    2017-05-23 03:55:27|  21.9|        12.69|2017|    5|\n|0005a1a1728c9d785...|            1|310ae3c140ff94b03...|a416b6a846a117243...|    2018-03-26 18:31:29|145.95|        11.65|2018|    3|\n|0005f50442cb953dc...|            1|4535b0e1091c278df...|ba143b05f0110f0dc...|    2018-07-06 14:10:56| 53.99|         11.4|2018|    7|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df5.dropDuplicates(['order_id', 'order_item_id']).show(5)"}, {"cell_type": "code", "execution_count": 13, "id": "7b4861b5-3211-4ae5-90bf-bafdca9b594c", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime| price|freight_value|year|month|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+\n|f5c5b9b21c9e6cf7f...|            1|0521fe3eb04940304...|e99e927c81e3f5173...|    2017-11-21 03:46:34|104.53|        17.21|2017|   11|\n|6e77d9428b5dec0e2...|            1|d1c427060a0f73f6b...|a1043bafd471dff53...|    2018-04-25 11:15:31| 119.0|        19.74|2018|    4|\n|64320e0f5e1ada4c0...|            1|7ce94ab189134e2d3...|8b321bb669392f516...|    2017-12-21 23:14:42| 13.65|         7.78|2017|   12|\n|9fe28378e5a0c6b3f...|            2|3938defa878985e56...|412a4720f3e9431b4...|    2018-07-19 04:31:15| 168.0|        60.91|2018|    7|\n|5697a446ff63940af...|            1|9c7140bb02241a583...|7722b1df1b0e383e0...|    2017-08-16 15:45:08|  25.9|         8.72|2017|    8|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 60:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime|price|freight_value|year|month|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n|638913622370c1dfe...|            1|8b50a72d52d7a91fb...|9f505651f4a6abe90...|    2018-08-02 11:45:14|49.99|         7.61|2018|    8|\n|845af9472cc76e66f...|            1|781afe929e3016a66...|08633c14ef2db992c...|    2017-12-19 09:30:52|107.9|        15.51|2017|   12|\n|13ac325ed34b96835...|            1|62dbbad1385feb9b2...|4d6d651bd7684af3f...|    2018-06-11 08:51:53|250.0|        20.72|2018|    6|\n|8e3cbe2ec233a68c1...|            1|c9dbe2eec19a8093c...|cca3071e3e9bb7d12...|    2017-10-19 21:28:18| 96.9|        17.93|2017|   10|\n|6649ebec1c6f3e185...|            4|42189544021ccb736...|b18dc380845b24038...|    2018-03-02 16:55:54|99.89|        16.46|2018|    3|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "24/08/27 03:53:21 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_57_piece0 !\n24/08/27 03:53:21 WARN BlockManagerMaster: Failed to remove broadcast 57 with removeFromMaster = true - org.apache.spark.SparkException: Could not find BlockManagerEndpoint1.\n\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:178)\n\tat org.apache.spark.rpc.netty.Dispatcher.postRemoteMessage(Dispatcher.scala:136)\n\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)\n\tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:163)\n\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:109)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\njava.lang.RuntimeException: org.apache.spark.SparkException: Could not find BlockManagerEndpoint1.\n\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:178)\n\tat org.apache.spark.rpc.netty.Dispatcher.postRemoteMessage(Dispatcher.scala:136)\n\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)\n\tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:163)\n\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:109)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:209) ~[spark-network-common_2.12-3.5.0.jar:3.5.0]\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142) ~[spark-network-common_2.12-3.5.0.jar:3.5.0]\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53) ~[spark-network-common_2.12-3.5.0.jar:3.5.0]\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) ~[netty-handler-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) ~[netty-codec-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102) ~[spark-network-common_2.12-3.5.0.jar:3.5.0]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n\tat java.base/java.lang.Thread.run(Thread.java:829) [?:?]\n24/08/27 03:53:21 ERROR ContextCleaner: Error cleaning broadcast 57\norg.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56) ~[spark-common-utils_2.12-3.5.0.jar:3.5.0]\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310) ~[spark-core_2.12-3.5.0.jar:3.5.0]\n\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75) ~[spark-core_2.12-3.5.0.jar:3.5.0]\n\tat org.apache.spark.storage.BlockManagerMaster.removeBroadcast(BlockManagerMaster.scala:222) ~[spark-core_2.12-3.5.0.jar:3.5.0]\n\tat org.apache.spark.broadcast.TorrentBroadcast$.unpersist(TorrentBroadcast.scala:395) ~[spark-core_2.12-3.5.0.jar:3.5.0]\n\tat org.apache.spark.broadcast.TorrentBroadcastFactory.unbroadcast(TorrentBroadcastFactory.scala:49) ~[spark-core_2.12-3.5.0.jar:3.5.0]\n\tat org.apache.spark.broadcast.BroadcastManager.unbroadcast(BroadcastManager.scala:82) ~[spark-core_2.12-3.5.0.jar:3.5.0]\n\tat org.apache.spark.ContextCleaner.doCleanupBroadcast(ContextCleaner.scala:256) ~[spark-core_2.12-3.5.0.jar:3.5.0]\n\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3(ContextCleaner.scala:204) ~[spark-core_2.12-3.5.0.jar:3.5.0]\n\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3$adapted(ContextCleaner.scala:195) ~[spark-core_2.12-3.5.0.jar:3.5.0]\n\tat scala.Option.foreach(Option.scala:407) ~[scala-library-2.12.18.jar:?]\n\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195) ~[spark-core_2.12-3.5.0.jar:3.5.0]\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1357) [spark-core_2.12-3.5.0.jar:3.5.0]\n\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189) [spark-core_2.12-3.5.0.jar:3.5.0]\n\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79) [spark-core_2.12-3.5.0.jar:3.5.0]\nCaused by: java.lang.RuntimeException: org.apache.spark.SparkException: Could not find BlockManagerEndpoint1.\n\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:178)\n\tat org.apache.spark.rpc.netty.Dispatcher.postRemoteMessage(Dispatcher.scala:136)\n\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)\n\tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:163)\n\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:109)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\n\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:209) ~[spark-network-common_2.12-3.5.0.jar:3.5.0]\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142) ~[spark-network-common_2.12-3.5.0.jar:3.5.0]\n\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53) ~[spark-network-common_2.12-3.5.0.jar:3.5.0]\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286) ~[netty-handler-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) ~[netty-codec-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102) ~[spark-network-common_2.12-3.5.0.jar:3.5.0]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562) ~[netty-transport-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.100.Final.jar:4.1.100.Final]\n\tat java.base/java.lang.Thread.run(Thread.java:829) ~[?:?]\n"}], "source": "df5.distinct().show(5)\n\ndf5.dropDuplicates().show(5)"}, {"cell_type": "code", "execution_count": 14, "id": "5a3089c5-6503-42a5-8386-2b54d0a46fc7", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime| price|freight_value|year|month|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+\n|0812eb902a67711a1...|            1|489ae2aa008f02150...|e3b4998c7a498169d...|    2017-02-16 20:37:36|6735.0|       194.31|2017|    2|\n|fefacc66af859508b...|            1|69c590f7ffc7bf8db...|80ceebb4ee9b31afb...|    2018-08-02 04:05:13|6729.0|       193.21|2018|    8|\n|f5136e38d1a14a4db...|            1|1bdf5e6731585cf01...|ee27a8f15b1dded4d...|    2017-06-15 02:45:17|6499.0|       227.66|2017|    6|\n|a96610ab360d42a2e...|            1|a6492cc69376c469a...|59417c56835dd8e2e...|    2017-04-18 13:25:18|4799.0|       151.34|2017|    4|\n|199af31afc78c699f...|            1|c3ed642d592594bb6...|59417c56835dd8e2e...|    2017-05-09 15:50:15|4690.0|        74.34|2017|    5|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 64:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime|price|freight_value|year|month|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\n|c5bdd8ef3c0ec4202...|            2|8a3254bee785a526d...|96804ea39d96eb908...|    2018-05-07 02:55:22| 0.85|         22.3|2018|    5|\n|3ee6513ae7ea23bdf...|            1|8a3254bee785a526d...|96804ea39d96eb908...|    2018-05-04 03:55:26| 0.85|        18.23|2018|    5|\n|6e864b3f0ec710311...|            1|8a3254bee785a526d...|96804ea39d96eb908...|    2018-05-02 20:30:34| 0.85|        18.23|2018|    5|\n|8272b63d03f5f79c5...|            1|270516a3f41dc035a...|2709af9587499e95e...|    2017-07-21 18:25:23|  1.2|         7.89|2017|    7|\n|8272b63d03f5f79c5...|            4|05b515fdc76e888aa...|2709af9587499e95e...|    2017-07-21 18:25:23|  1.2|         7.89|2017|    7|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df5.orderBy(col('price').desc()).show(5)\n\ndf5.orderBy(col('price').asc(), col('freight_value').desc()).show(5)"}, {"cell_type": "code", "execution_count": 15, "id": "e3681484-1926-4667-9e88-14d59796b062", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+----+-----------+------------------+-----------------+---------+---------+\n|year|total_count|         avg_price|        sum_price|min_price|max_price|\n+----+-----------+------------------+-----------------+---------+---------+\n|2018|      62511|120.08515685239729|7506643.240000207|     0.85|   6729.0|\n|2017|      49765|121.26732804179925| 6034868.58000014|      1.2|   6735.0|\n|2016|        370|134.55654054054054|         49785.92|      6.0|   1399.0|\n|2020|          4|             86.49|           345.96|    69.99|    99.99|\n+----+-----------+------------------+-----------------+---------+---------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 75:===================================================>     (9 + 1) / 10]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----+-----+-----------+------------------+------------------+---------+---------+\n|year|month|total_count|         avg_price|         sum_price|min_price|max_price|\n+----+-----+-----------+------------------+------------------+---------+---------+\n|2016|    9|          4| 48.61750000000001|194.47000000000003|    44.99|     59.5|\n|2016|   10|        365|135.83712328767123|49580.549999999996|      6.0|   1399.0|\n|2016|   12|          1|              10.9|              10.9|     10.9|     10.9|\n|2017|    1|        681|117.65747430249625| 80124.73999999995|      2.9|   1999.0|\n|2017|    2|       1866| 131.8231564844589|245982.01000000033|      3.9|   6735.0|\n+----+-----+-----------+------------------+------------------+---------+---------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "\ndf5.groupBy('year').agg(count('*').alias('total_count'), \n                        avg('price').alias('avg_price'), \n                        sum('price').alias('sum_price'),\n                        min('price').alias('min_price'),\n                        max('price').alias('max_price')).show(5)\n\n\ndf5.groupBy('year', 'month').agg(count('*').alias('total_count'), \n                        avg('price').alias('avg_price'), \n                        sum('price').alias('sum_price'),\n                        min('price').alias('min_price'),\n                        max('price').alias('max_price')).orderBy(col('year').asc(),col('month').asc()).show(5)"}, {"cell_type": "code", "execution_count": 16, "id": "2cb30a44-2b00-47b3-bf71-0a7c66ca655f", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 79:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime| price|freight_value|year|month|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+\n|1e1bb536916a99649...|            2|0288f8dd74b931b4e...|1da3aeb70d7989d1e...|    2017-09-05 12:10:11| 49.99|        21.15|2017|    9|\n|62a0e822dd605871a...|            1|31dbb0d1815bdc83c...|6da1992f915d77be9...|    2017-06-08 11:50:18|  29.0|        15.79|2017|    6|\n|025c72e88fbf2358b...|            2|bef21943bc2335188...|e49c26c3edfa46d22...|    2017-03-21 21:24:27|  19.9|         20.8|2017|    3|\n|23d16dddab46fd3d0...|            1|cca8e09ba6f2d35e4...|43f8c9950d11ecd03...|    2018-01-31 22:17:51|109.99|        14.52|2018|    1|\n|71c0d1686c9b55563...|            2|eb6c2ecde53034fc9...|1025f0e2d44d7041d...|    2017-12-01 19:31:45| 32.99|        16.11|2017|   12|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df5.fillna({'price': 0, 'freight_value': 0}).show(5)"}, {"cell_type": "code", "execution_count": 17, "id": "0545c141-73da-4230-8ba0-f4cf7327455c", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 84:===================================================>     (9 + 1) / 10]\r"}, {"name": "stdout", "output_type": "stream", "text": "13591643.699999437\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "accum=spark.sparkContext.accumulator(0)\n\ndf5.foreach(lambda row: accum.add(row['price']))\nprint(accum.value) #Accessed by driver"}, {"cell_type": "code", "execution_count": 18, "id": "51b21919-31aa-40c5-9a6e-c7ca0d816275", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 85:===========================================>              (3 + 1) / 4]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+--------------+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime|price|freight_value|year|month|price_category|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+--------------+\n|cfe5da82479414589...|            1|6f3b5b605d91b7439...|4869f7a5dfa277a7d...|    2018-08-06 22:15:29|159.9|        13.78|2018|    8|          High|\n|af3ad705f2a257c86...|            1|694e3712264584430...|744dac408745240a2...|    2017-11-20 15:11:01|249.0|        26.77|2017|   11|          High|\n|c2eaa6cab239c2145...|            1|bbdb487c5f9a780a6...|aaed1309374718fdd...|    2017-04-27 15:15:20| 99.9|        26.96|2017|    4|        Medium|\n|e86d50789574a28a9...|            1|00d93a09990b319a7...|391fc6631aebcf300...|    2017-11-30 12:13:41|169.9|        26.03|2017|   11|          High|\n|bc3ec72d51f1b2677...|            1|a668879a637b097eb...|955fee9216a65b617...|    2018-05-07 15:35:23|110.0|         9.12|2018|    5|          High|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+--------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df5.withColumn(\"price_category\",  when(col('price') >= 100, \"High\")\n                                 .when((col('price') < 100) & (col('price') >= 50), \"Medium\")\n                                 .otherwise(\"Low\")).show(5)"}, {"cell_type": "code", "execution_count": 19, "id": "8b0d8f94-ff71-4148-8b19-2a9cfba02d2d", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+----------+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime|price|freight_value|year|month|dense_rank|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+----------+\n|3ee6513ae7ea23bdf...|            1|8a3254bee785a526d...|96804ea39d96eb908...|    2018-05-04 03:55:26| 0.85|        18.23|2018|    5|         1|\n|6e864b3f0ec710311...|            1|8a3254bee785a526d...|96804ea39d96eb908...|    2018-05-02 20:30:34| 0.85|        18.23|2018|    5|         1|\n|c5bdd8ef3c0ec4202...|            2|8a3254bee785a526d...|96804ea39d96eb908...|    2018-05-07 02:55:22| 0.85|         22.3|2018|    5|         1|\n|f1d5c2e6867fa93ce...|            1|46fce52cef5caa7cc...|2d2322d8421188677...|    2018-08-28 21:30:15|  2.2|         7.39|2018|    8|         2|\n|de03f4f4bb610147a...|            1|2e8316b31db34314f...|9d7a1d34a50524090...|    2018-02-08 00:55:32| 2.99|        11.85|2018|    2|         3|\n+--------------------+-------------+--------------------+--------------------+-----------------------+-----+-------------+----+-----+----------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 99:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+------------------+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime| price|freight_value|year|month|       running_sum|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+------------------+\n|b2d1902261f105c5c...|            1|5ed3835ea6f96c77b...|aba1721a889e04dec...|    2018-01-01 22:08:31| 139.0|         8.23|2018|    1|             139.0|\n|3c8e80909dd1066fd...|            1|4308439e0d80d5fe0...|59fb871bf6f4522a8...|    2018-01-01 22:13:24|179.99|         27.8|2018|    1|            318.99|\n|f2e5bcbd102cd01f1...|            1|2bb3e85f2a403543f...|76d64c4aca3a7baf2...|    2018-01-01 22:27:15| 348.9|       118.06|2018|    1|            667.89|\n|2e7080c8c24e4a977...|            1|3bdc89e963c6651b8...|fffd5413c0700ac82...|    2018-01-01 22:30:19| 262.5|       115.49|2018|    1|            930.39|\n|43ac8d91a0749b176...|            1|8dad29b3b6c42d0b9...|4e922959ae960d389...|    2018-01-01 22:36:29| 129.0|        13.92|2018|    1|1059.3899999999999|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.window import Window\n\nwindowSpec1 = Window.partitionBy('year').orderBy(col('price').asc())\ndf5.withColumn('dense_rank', dense_rank().over(windowSpec1)).show(5)\n\nwindowSpec2 = Window.partitionBy('year').orderBy(col('shipping_limit_datetime').asc())\ndf5.withColumn('running_sum', sum('price').over(windowSpec2)).show(5)"}, {"cell_type": "code", "execution_count": 20, "id": "02132e35-1045-4b66-83c1-09889ce99a01", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "root\n |-- seller_id: string (nullable = true)\n |-- seller_zip_code_prefix: integer (nullable = true)\n |-- seller_city: string (nullable = true)\n |-- seller_state: string (nullable = true)\n\n+--------------------+----------------------+-----------------+------------+\n|           seller_id|seller_zip_code_prefix|      seller_city|seller_state|\n+--------------------+----------------------+-----------------+------------+\n|3442f8959a84dea7e...|                 13023|         campinas|          SP|\n|d1b65fc7debc3361e...|                 13844|       mogi guacu|          SP|\n|ce3ad9de960102d06...|                 20031|   rio de janeiro|          RJ|\n|c0f3eea2e14555b6f...|                  4195|        sao paulo|          SP|\n|51a04a8a6bdcb23de...|                 12914|braganca paulista|          SP|\n+--------------------+----------------------+-----------------+------------+\nonly showing top 5 rows\n\n"}], "source": "hdfs_path = '/tmp/input_data/sellers_dataset.csv'\nsdf = spark.read.format('csv').option('header', 'true').option('inferSchema', 'true').load(hdfs_path)\n\n\nsdf.printSchema()\nsdf.show(5)"}, {"cell_type": "code", "execution_count": 21, "id": "0abd3286-56ed-493b-841a-e63eb0285ad4", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+----------------------+-----------+------------+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime| price|freight_value|year|month|seller_zip_code_prefix|seller_city|seller_state|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+----------------------+-----------+------------+\n|1e1bb536916a99649...|            2|0288f8dd74b931b4e...|1da3aeb70d7989d1e...|    2017-09-05 12:10:11| 49.99|        21.15|2017|    9|                  4265|  sao paulo|          SP|\n|62a0e822dd605871a...|            1|31dbb0d1815bdc83c...|6da1992f915d77be9...|    2017-06-08 11:50:18|  29.0|        15.79|2017|    6|                  1026|  sao paulo|          SP|\n|025c72e88fbf2358b...|            2|bef21943bc2335188...|e49c26c3edfa46d22...|    2017-03-21 21:24:27|  19.9|         20.8|2017|    3|                 55325|     brejao|          PE|\n|23d16dddab46fd3d0...|            1|cca8e09ba6f2d35e4...|43f8c9950d11ecd03...|    2018-01-31 22:17:51|109.99|        14.52|2018|    1|                  6341|carapicuiba|          SP|\n|71c0d1686c9b55563...|            2|eb6c2ecde53034fc9...|1025f0e2d44d7041d...|    2017-12-01 19:31:45| 32.99|        16.11|2017|   12|                  3204|  sao paulo|          SP|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+----------------------+-----------+------------+\nonly showing top 5 rows\n\n"}], "source": "result1 = df5.join(broadcast(sdf), df5.seller_id == sdf.seller_id, 'inner').drop(sdf.seller_id)\n\nresult1.show(5)"}, {"cell_type": "code", "execution_count": 22, "id": "d1e0eed3-f28e-4ca4-91c8-0ea64db14e4c", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 107:============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+----------------------+-----------+------------+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime| price|freight_value|year|month|seller_zip_code_prefix|seller_city|seller_state|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+----------------------+-----------+------------+\n|1e1bb536916a99649...|            2|0288f8dd74b931b4e...|1da3aeb70d7989d1e...|    2017-09-05 12:10:11| 49.99|        21.15|2017|    9|                  4265|  sao paulo|          SP|\n|62a0e822dd605871a...|            1|31dbb0d1815bdc83c...|6da1992f915d77be9...|    2017-06-08 11:50:18|  29.0|        15.79|2017|    6|                  1026|  sao paulo|          SP|\n|025c72e88fbf2358b...|            2|bef21943bc2335188...|e49c26c3edfa46d22...|    2017-03-21 21:24:27|  19.9|         20.8|2017|    3|                 55325|     brejao|          PE|\n|23d16dddab46fd3d0...|            1|cca8e09ba6f2d35e4...|43f8c9950d11ecd03...|    2018-01-31 22:17:51|109.99|        14.52|2018|    1|                  6341|carapicuiba|          SP|\n|71c0d1686c9b55563...|            2|eb6c2ecde53034fc9...|1025f0e2d44d7041d...|    2017-12-01 19:31:45| 32.99|        16.11|2017|   12|                  3204|  sao paulo|          SP|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+----------------------+-----------+------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "result2 = df5.alias('oid').join(sdf.alias('sid'), col('oid.seller_id') == col('sid.seller_id'), 'inner').drop(col('sid.seller_id'))\n\nresult2.show(5)"}, {"cell_type": "code", "execution_count": 24, "id": "ad617140-e304-4eb5-ac02-1789149e76b6", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+--------------------+----------------------+-----------+------------+\n|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_datetime| price|freight_value|year|month|           seller_id|seller_zip_code_prefix|seller_city|seller_state|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+--------------------+----------------------+-----------+------------+\n|1e1bb536916a99649...|            2|0288f8dd74b931b4e...|1da3aeb70d7989d1e...|    2017-09-05 12:10:11| 49.99|        21.15|2017|    9|1da3aeb70d7989d1e...|                  4265|  sao paulo|          SP|\n|62a0e822dd605871a...|            1|31dbb0d1815bdc83c...|6da1992f915d77be9...|    2017-06-08 11:50:18|  29.0|        15.79|2017|    6|6da1992f915d77be9...|                  1026|  sao paulo|          SP|\n|025c72e88fbf2358b...|            2|bef21943bc2335188...|e49c26c3edfa46d22...|    2017-03-21 21:24:27|  19.9|         20.8|2017|    3|e49c26c3edfa46d22...|                 55325|     brejao|          PE|\n|23d16dddab46fd3d0...|            1|cca8e09ba6f2d35e4...|43f8c9950d11ecd03...|    2018-01-31 22:17:51|109.99|        14.52|2018|    1|43f8c9950d11ecd03...|                  6341|carapicuiba|          SP|\n|71c0d1686c9b55563...|            2|eb6c2ecde53034fc9...|1025f0e2d44d7041d...|    2017-12-01 19:31:45| 32.99|        16.11|2017|   12|1025f0e2d44d7041d...|                  3204|  sao paulo|          SP|\n+--------------------+-------------+--------------------+--------------------+-----------------------+------+-------------+----+-----+--------------------+----------------------+-----------+------------+\nonly showing top 5 rows\n\n"}], "source": "df5.createOrReplaceTempView(\"ORDER_ITEM\")\nsdf.createOrReplaceTempView(\"SELLERS\")\n\n\njoinDF2 = spark.sql(\"select * from ORDER_ITEM oid INNER JOIN SELLERS sid ON oid.seller_id == sid.seller_id\")\n\njoinDF2.show(5)"}, {"cell_type": "code", "execution_count": 25, "id": "8390c432-ece3-45e2-8cdb-27cf31e27a6a", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 118:==================================================>     (9 + 1) / 10]\r"}, {"name": "stdout", "output_type": "stream", "text": "Write Successfull\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "result1.write.format('csv').option('header', 'true').option('delimiter', ',').save('/tmp/output_data/result1/')\nprint(\"Write Successfull\")"}, {"cell_type": "code", "execution_count": 26, "id": "d7878e19-e56a-4fce-854c-2c9849f0c013", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 122:==================================================>     (9 + 1) / 10]\r"}, {"name": "stdout", "output_type": "stream", "text": "Write Successfull\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "result1.write.partitionBy('year').format('csv').option('header', 'true').option('delimiter', ',').save('/tmp/output_data/result2/')\nprint(\"Write Successfull\")"}, {"cell_type": "code", "execution_count": 27, "id": "00e1ecd7-3bd6-4d79-a812-19855e792e4a", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 126:>                                                        (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "Write Successfull\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "result1.coalesce(1).write.format('csv').option('header', 'true').option('delimiter', ',').save('/tmp/output_data/result3/')\nprint(\"Write Successfull\")"}, {"cell_type": "code", "execution_count": 28, "id": "aae7ef7a-171f-4ac8-92c3-86b49f709995", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "24/08/27 04:42:31 WARN SetCommand: 'SET hive.exec.dynamic.partition.mode=nonstrict' might not work, since Spark doesn't support changing the Hive config dynamically. Please pass the Hive-specific config by adding the prefix spark.hadoop (e.g. spark.hadoop.hive.exec.dynamic.partition.mode) when starting a Spark application. For details, see the link: https://spark.apache.org/docs/latest/configuration.html#dynamically-loading-spark-properties.\nivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n24/08/27 04:42:33 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n24/08/27 04:42:33 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "write successfull\n"}], "source": "spark.sql(\"\"\"set hive.exec.dynamic.partition.mode=nonstrict\"\"\")\n\nspark.sql(\"\"\"USE tables_by_spark\"\"\")\n\nspark.sql(\"\"\"\n    CREATE TABLE IF NOT EXISTS order_sellers_data (\n        order_id STRING,\n        order_item_id INT,\n        product_id STRING,\n        price DOUBLE,\n        freight_value DOUBLE,\n        seller_city STRING\n    ) PARTITIONED BY (year INT)\n\"\"\")\n\n\nresult1.select('order_id',\n              'order_item_id',\n              'product_id',\n              'price',\n              'freight_value',\n              'seller_city',\n              'year').write.mode(\"append\").insertInto(\"order_sellers_data\")\nprint(\"write successfull\")"}, {"cell_type": "code", "execution_count": null, "id": "958d8abd-18aa-4c2d-a8f6-5645266be751", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}